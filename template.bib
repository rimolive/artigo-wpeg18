% This file was created with JabRef 2.10.
% Encoding: Cp1252

@book{Witten:2011,
	abstract = {"Data Mining: Practical Machine Learning Tools and Techniques" offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Witten, Ian H and Frank, Eibe and Hall, Mark a},
	booktitle = {Complementary literature None},
	doi = {0120884070, 9780120884070},
	eprint = {arXiv:1011.1669v3},
	isbn = {0080890369},
	issn = {14337851},
	pmid = {11221713},
	title = {{Data Mining: Practical Machine Learning Tools and Techniques (Google eBook)}},
	year = {2011}
}

@article{Crosas:2015,
	abstract = {The vast majority of social science research presently uses small (MB or GB scale) data sets. These fixed-scale data sets are commonly downloaded to the researcher's computer where the analysis is performed locally, and are often shared and cited with well-established technologies, such as the Dataverse Project (see Dataverse.org), to support the published results. The trend towards Big Data -- including large scale streaming data -- is starting to transform research and has the potential to impact policy-making and our understanding of the social, economic, and political problems that affect human societies. However, this research poses new challenges in execution, accountability, preservation, reuse, and reproducibility. Downloading these data sets to a researcher's computer is infeasible or not practical; hence, analyses take place in the cloud, require unusual expertise, and benefit from collaborative teamwork and novel tool development. The advantage of these data sets in how informative they are also means that they are much more likely to contain highly sensitive personally identifiable information. In this paper, we discuss solutions to these new challenges so that the social sciences can realize the potential of Big Data.},
	author = {Crosas, Merc{\`{e}} and King, Gary and Honaker, James and Sweeney, Latanya},
	doi = {10.1177/0002716215570847},
	isbn = {0002716215570},
	issn = {15523349},
	journal = {Annals of the American Academy of Political and Social Science},
	keywords = {archive,big data,big data algorithms,big data methods,data privacy,differential privacy,repository},
	number = {1},
	pages = {260--273},
	title = {{Automating Open Science for Big Data}},
	volume = {659},
	year = {2015}
}

@article{Fu:2017,
	abstract = {"Big Data" is on the covers of  Science, Nature, the  Economist, and  Wired magazines, on the front pages of the  Wall Street Journal and the  New York Times. But despite the media hyperbole, as Christine Borgman points out in this examination of data and scholarly research, having the right data is usually better than having more data; little data can be just as valuable as big data. In many cases, there are no data -- because relevant data don't exist, cannot be found, or are not available. Moreover, data sharing is difficult, incentives to do so are minimal, and data practices vary widely across disciplines.Borgman, an often-cited authority on scholarly communication, argues that data have no value or meaning in isolation; they exist within a knowledge infrastructure -- an ecology of people, practices, technologies, institutions, material objects, and relationships. After laying out the premises of her investigation -- six "provocations" meant to inspire discussion about the uses of data in scholarship -- Borgman offers case studies of data practices in the sciences, the social sciences, and the humanities, and then considers the implications of her findings for scholarly practice and research policy. To manage and exploit data over the long term, Borgman argues, requires massive investment in knowledge infrastructures; at stake is the future of scholarship.},
	archivePrefix = {arXiv},
	arxivId = {0803.1716},
	author = {Fu, Tao},
	doi = {10.1080/1369118X.2017.1344286},
	eprint = {0803.1716},
	isbn = {978-0-262-02856-1},
	issn = {1369-118X},
	journal = {Information, Communication {\&} Society},
	number = {12},
	pages = {1815--1816},
	pmid = {502955140},
	title = {{Big Data, Little Data, No Data: Scholarship in the Networked World}},
	url = {https://www.tandfonline.com/doi/full/10.1080/1369118X.2017.1344286},
	volume = {20},
	year = {2017}
}

@article{Peterson:2015,
	abstract = {Biodiversity informatics is a field that is growing rapidly in data infrastructure, tools, and participation by researchers worldwide from diverse disciplines and with diverse, innovative approaches. A recent 'decadal view' of the field laid out a vision that was nonetheless restricted and constrained by its European focus. Our alternative decadal view is global, i.e., it sees the worldwide scope and importance of biodiversity informatics as addressing five major, global goals: (1) mobilize existing knowledge; (2) share this knowledge and the experience of its myriad deployments globally; (3) avoid 'siloing' and reinventing the tools of knowledge deployment; (4) tackle biodiversity informatics challenges at appropriate scales; and (5) seek solutions to difficult challenges that are strategic.},
	author = {Peterson, A. Townsend and Sober{\'{o}}n, Jorge and Krishtalka, Leonard},
	doi = {10.1186/s12898-015-0046-8},
	isbn = {1472-6785},
	issn = {14726785},
	journal = {BMC Ecology},
	keywords = {Biodiversity informatics,Capacity-building,Data,Infrastructure,Training},
	number = {1},
	pmid = {26022532},
	title = {{A global perspective on decadal challenges and priorities in biodiversity informatics}},
	volume = {15},
	year = {2015}
}

@book{White:2015,
	abstract = {Hadoop: The Definitive Guide helps you harness the power of your data. Ideal for processing large datasets, the Apache Hadoop framework is an open source implementation of the MapReduce algorithm on which Google built its empire. This comprehensive resource demonstrates how to use Hadoop to build reliable, scalable, distributed systems: programmers will find details for analyzing large datasets, and administrators will learn how to set up and run Hadoop clusters. Complete with case studies that illustrate how Hadoop solves specific problems, this book helps you: Use the Hadoop Distributed File System (HDFS) for storing large datasets, and run distributed computations over those datasets using MapReduce Become familiar with Hadoop's data and I/O building blocks for compression, data integrity, serialization, and persistence Discover common pitfalls and advanced features for writing real-world MapReduce programs Design, build, and administer a dedicated Hadoop cluster, or run Hadoop in the cloud Use Pig, a high-level query language for large-scale data processing Take advantage of HBase, Hadoop's database for structured and semi-structured data Learn ZooKeeper, a toolkit of coordination primitives for building distributed systems If you have lots of data - whether it's gigabytes or petabytes - Hadoop is the perfect solution. Hadoop: The Definitive Guide is the most thorough book available on the subject. "Now you have the opportunity to learn about Hadoop from a master-not only of the technology, but also of common sense and plain talk." - Doug Cutting, Hadoop Founder, Yahoo!},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {White, Tom},
	booktitle = {Online},
	doi = {citeulike-article-id:4882841},
	eprint = {arXiv:1011.1669v3},
	isbn = {9781449311520},
	issn = {1947-4040},
	pmid = {16323778},
	title = {{Hadoop: The definitive guide}},
	year = {2015}
}

@book{Marquesone:2014,
	author = {Marquesone, Rosangela},
	title = {{Big Data: T{\'{e}}cnicas e tecnologias para extra{\c{c}}{\~{a}}o de valor dos dados}},
	year = {2014}
}


@article{Hashem:2015,
	abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized. {\textcopyright} 2014 Elsevier Ltd.},
	author = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
	doi = {10.1016/j.is.2014.07.006},
	file = {:home/rmartine/Documents/Poli/1-s2.0-S0306437914001288-main.pdf:pdf},
	isbn = {0306-4379},
	issn = {03064379},
	journal = {Information Systems},
	keywords = {Big data,Cloud computing,Hadoop},
	pmid = {1476196123},
	title = {{The rise of "big data" on cloud computing: Review and open research issues}},
	year = {2015}
}

@online{ApacheSpark,
	title = {Apache Spark project},
	url = {http://spark.apache.org}
}

@online{ArmProject,
	title = {ARM Project},
	url = {http://www.arm.gov}
}

@online{ApacheCassandra,
	title = {Apache Cassandra project},
	url = {http://cassandra.apache.org}
}

@online{RadanalyticsIo,
	title = {RADAnalytics.io project},
	url = {https://radnalytics.io}
}

@online{ApacheZeppelin,
	title = {Apache Zeppelin project},
	url = {http://zeppelin.apache.org}
}

@online{Gbif,
    title = {Global Biodiversity Information Facility},
    url = {http://www.gbif.org/}
}